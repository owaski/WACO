{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install praat-textgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from string import punctuation\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sentencepiece\n",
    "import torchaudio\n",
    "import textgrids\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "from fairseq.data import PhonemeDictionary\n",
    "from WACO.prepare_data.data_utils import load_df_from_tsv, save_df_to_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = None\n",
    "lang = 'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm = sentencepiece.SentencePieceProcessor(os.path.join(root, 'spm_unigram10000_st_{}.model'.format(lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, '{}_st_{}.tsv'.format(split, lang)))\n",
    "# df = load_df_from_tsv(os.path.join(root, 'train-1h_asr_10h.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479.9419181770833"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_frames'].sum() / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8958341319444445"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_frames = 0\n",
    "for fn in os.listdir('{}/en-de/data/dev/wav'.format(root)):\n",
    "    info = torchaudio.info('{}/en-de/data/dev/wav/'.format(root) + fn)\n",
    "    n_frames += info.num_frames\n",
    "n_frames / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('{}/en-de/data/train/txt/train.yaml'.format(root)) as r:\n",
    "    y = yaml.load(r, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.0411619297576"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration = 0\n",
    "for x in y:\n",
    "    duration += x['duration']\n",
    "duration / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = list(range(train_df.shape[0]))\n",
    "# random.shuffle(indices)\n",
    "# save_df_to_tsv(train_df.iloc[indices[:10000]], os.path.join(root, 'train-tiny_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, 'en-{}'.format(lang), 'data', split, 'align_mfat_10h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418/1418 [00:02<00:00, 627.47it/s]\n"
     ]
    }
   ],
   "source": [
    "last_audio_path = None\n",
    "for idx in tqdm(range(len(df))):\n",
    "    audio_path, offset, num_frames = os.path.join(root, df['audio'][idx]).split(':')\n",
    "    offset, num_frames = int(offset), int(num_frames)\n",
    "    if last_audio_path is None or audio_path != last_audio_path:\n",
    "        waveform, frame_rate = torchaudio.load(os.path.join(root, audio_path))\n",
    "        last_audio_path = audio_path\n",
    "    torchaudio.save(os.path.join(save_dir, '{}.wav'.format(df['id'][idx])), waveform[:, offset : offset + num_frames], sample_rate=frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['src_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418/1418 [00:00<00:00, 17489.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def covered(s, punctuation):\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "space = '▁'\n",
    "tokenized_sentences = []\n",
    "segmentss = []\n",
    "punctuation = punctuation + '—’'\n",
    "for sent in tqdm(df['src_text'].tolist()):\n",
    "    tokens = spm.EncodeAsPieces(sent)\n",
    "    segments = []\n",
    "    last = -1\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.startswith(space) or covered(token, punctuation):\n",
    "            if last != -1 and last <= idx - 1:\n",
    "                segments.append((last, idx - 1))\n",
    "            last = idx + (token == space or covered(token, punctuation) or \\\n",
    "                (token.startswith(space) and len(token) > 1 and covered(token[1:], punctuation)))    \n",
    "    \n",
    "    if last < len(tokens):\n",
    "        segments.append((last, len(tokens) - 1))\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    for seg in segments:\n",
    "        token = ''.join(tokens[seg[0] : seg[1] + 1]).replace(space, '')\n",
    "        if token.replace(',', '').isnumeric():\n",
    "            token = token.replace(',', '')\n",
    "        tokenized_sentence.append(token)\n",
    "\n",
    "    tokenized_sentences.append(tokenized_sentence)\n",
    "    segmentss.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418/1418 [00:00<00:00, 29121.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    with open(os.path.join(save_dir, '{}.txt'.format(id)), 'w') as w:\n",
    "        w.write(' '.join(tokenized_sentences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "mfa align . english_mfa english_mfa textgrids --clean\n",
    "mfa train -o model/acoustic_model --phone_set IPA --output_format long_textgrid --include_original_text -t cache/MFA/mfat_enes ./ english_mfa ./textgrids --clean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1418/1418 [00:00<00:00, 2190.55it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(save_dir, 'textgrids/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        if len(filtered_grid) != len(tokenized_sentences[i]):\n",
    "            # print(i, [w.text for w in filtered_grid], tokenized_sentences[i], sep='\\n')\n",
    "            n_outlier += 1\n",
    "            continue\n",
    "\n",
    "        interval = np.array([(word.xmin, word.xmax) for word in filtered_grid])\n",
    "        audio_path = os.path.join(save_dir, '{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        th.save([segmentss[i], interval], os.path.join(save_dir, '{}.pt'.format(id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007768005814467718"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_outlier / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "for i, id in enumerate(tqdm(df['id'])):\n",
    "    grid_path = os.path.join(save_dir, 'textgrids/{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "\n",
    "        phones = [phone.text if phone.text != '' else '<empty>' for phone in grid['phones']]\n",
    "\n",
    "        interval = np.array([(phone.xmin, phone.xmax) for phone in grid['phones']])\n",
    "        audio_path = os.path.join(save_dir, '{}.wav'.format(id))\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        th.save([segmentss[i], interval], os.path.join(save_dir, '{}.phone.pt'.format(id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/phone.txt'.format(root), 'r') as r:\n",
    "    all_phones = [p.strip() for p in r.readlines() if p.strip() != '']\n",
    "    phone_dict = {p : i for i, p in enumerate(all_phones)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p = G2p()\n",
    "src_dict = PhonemeDictionary.load(os.path.join(root, 'phonemes.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_ls960_asr.tsv: 100%|██████████| 281241/281241 [10:31<00:00, 445.27it/s]\n",
      "dev_ls960_asr.tsv: 100%|██████████| 5567/5567 [00:07<00:00, 721.19it/s]\n"
     ]
    }
   ],
   "source": [
    "for fn in os.listdir(root):\n",
    "    # if fn.endswith('tsv') and ('asr' in fn or 'de.' in fn):\n",
    "    if 'ls' in fn:\n",
    "\n",
    "        df = load_df_from_tsv(os.path.join(root, fn))\n",
    "        list_of_phonemes = []\n",
    "        for src_text in tqdm(df['src_text'], desc=fn):\n",
    "            raw_phonemes = g2p(src_text)\n",
    "            phonemes = []\n",
    "            for idx in range(len(raw_phonemes)):\n",
    "                if raw_phonemes[idx] in src_dict:\n",
    "                    p = raw_phonemes[idx]\n",
    "                    if idx == 0 or raw_phonemes[idx - 1] not in src_dict:\n",
    "                        p = '▁' + p\n",
    "                    phonemes.append(p)\n",
    "            list_of_phonemes.append(' '.join(phonemes))\n",
    "        df['src_phoneme'] = list_of_phonemes\n",
    "        save_df_to_tsv(df, os.path.join(root, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5567/5567 [00:00<00:00, 18686.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# For Librispeech\n",
    "\n",
    "df = load_df_from_tsv('{}/dev_ls960_asr.tsv'.format(root))\n",
    "\n",
    "def covered(s, punctuation):\n",
    "    for c in s:\n",
    "        if c not in punctuation:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "space = '▁'\n",
    "tokenized_sentences = []\n",
    "segmentss = []\n",
    "punctuation = punctuation + '—’'\n",
    "punctuation = punctuation.replace(\"'\", '')\n",
    "for sent in tqdm(df['src_text'].tolist()):\n",
    "    tokens = spm.EncodeAsPieces(sent)\n",
    "    segments = []\n",
    "    last = -1\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token.startswith(space) or covered(token, punctuation):\n",
    "            if last != -1 and last <= idx - 1:\n",
    "                segments.append((last, idx - 1))\n",
    "            last = idx + (token == space or covered(token, punctuation) or \\\n",
    "                (token.startswith(space) and len(token) > 1 and covered(token[1:], punctuation)))    \n",
    "    \n",
    "    if last < len(tokens):\n",
    "        segments.append((last, len(tokens) - 1))\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    for seg in segments:\n",
    "        token = ''.join(tokens[seg[0] : seg[1] + 1]).replace(space, '')\n",
    "        if token.replace(',', '').isnumeric():\n",
    "            token = token.replace(',', '')\n",
    "        tokenized_sentence.append(token)\n",
    "\n",
    "    tokenized_sentences.append(tokenized_sentence)\n",
    "    segmentss.append(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00: 100%|██████████| 5567/5567 [00:06<00:00, 895.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014370396982216634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_grids = []\n",
    "n_outlier = 0\n",
    "iterator = tqdm(df['id'])\n",
    "for i, id in enumerate(iterator):\n",
    "    grid_path = os.path.join('{}/LibriSpeech/librispeech_mfa/textgrids/{}'.format(ls_root, df['speaker'][i]), '{}.TextGrid'.format(id))\n",
    "    if os.path.exists(grid_path):\n",
    "        grid = textgrids.TextGrid(grid_path)\n",
    "        filtered_grid = [tok for tok in grid['words'] if tok.text != '']\n",
    "\n",
    "        u = v = 0\n",
    "        intervals = []\n",
    "        fail = False\n",
    "        while u < len(filtered_grid) and v < len(tokenized_sentences[i]):\n",
    "            if filtered_grid[u].text == tokenized_sentences[i][v]:\n",
    "                intervals.append((filtered_grid[u].xmin, filtered_grid[u].xmax))\n",
    "                u += 1\n",
    "                v += 1\n",
    "            elif tokenized_sentences[i][v].startswith(filtered_grid[u].text):\n",
    "                if u < len(filtered_grid) - 1 and tokenized_sentences[i][v] == filtered_grid[u].text + filtered_grid[u + 1].text:\n",
    "                    intervals.append((filtered_grid[u].xmin, filtered_grid[u + 1].xmax))\n",
    "                    u += 2\n",
    "                    v += 1\n",
    "                else:\n",
    "                    fail = True\n",
    "                    break\n",
    "            else:\n",
    "                fail = True\n",
    "                break\n",
    "        \n",
    "        if u < len(filtered_grid) or v < len(tokenized_sentences[i]):\n",
    "            fail = True\n",
    "\n",
    "        iterator.set_description('{:.2f}'.format(n_outlier / (i + 1)))\n",
    "\n",
    "        if fail:\n",
    "            # print(i, [w.text for w in filtered_grid], tokenized_sentences[i], sep='\\n')\n",
    "            # break\n",
    "            n_outlier += 1\n",
    "            continue\n",
    "            \n",
    "\n",
    "        interval = np.array(intervals)\n",
    "        \n",
    "        audio_path = os.path.join(ls_root, df['audio'][i])\n",
    "        info = torchaudio.info(audio_path)\n",
    "        duration = info.num_frames / info.sample_rate\n",
    "        interval = interval / duration\n",
    "\n",
    "        assert len(segmentss[i]) == len(interval)\n",
    "        th.save([segmentss[i], interval], os.path.join('{}/LibriSpeech/librispeech_mfa/{}'.format(ls_root, df['speaker'][i]), '{}.pt'.format(id)))\n",
    "print(n_outlier / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03123891, 0.05679801],\n",
       "       [0.05679801, 0.07880724],\n",
       "       [0.10365637, 0.12992545],\n",
       "       [0.12992545, 0.15264466],\n",
       "       [0.15264466, 0.17607384],\n",
       "       [0.18033369, 0.19027334],\n",
       "       [0.19027334, 0.23358182],\n",
       "       [0.3542776 , 0.38125666],\n",
       "       [0.38125666, 0.40397586],\n",
       "       [0.40397586, 0.43237487],\n",
       "       [0.44373447, 0.46077387],\n",
       "       [0.46645367, 0.49272275],\n",
       "       [0.49272275, 0.50479233],\n",
       "       [0.50479233, 0.51828186],\n",
       "       [0.51828186, 0.53319134],\n",
       "       [0.53319134, 0.58146965],\n",
       "       [0.58146965, 0.60773873],\n",
       "       [0.62193823, 0.63968761],\n",
       "       [0.64820731, 0.66950657],\n",
       "       [0.66950657, 0.68512602],\n",
       "       [0.68512602, 0.68938587],\n",
       "       [0.68938587, 0.70642528],\n",
       "       [0.70642528, 0.73837416],\n",
       "       [0.76038339, 0.79233227],\n",
       "       [0.79233227, 0.80085197],\n",
       "       [0.80085197, 0.8342208 ],\n",
       "       [0.8342208 , 0.8455804 ],\n",
       "       [0.8455804 , 0.87042953],\n",
       "       [0.87042953, 0.90876819],\n",
       "       [0.90876819, 0.91444799],\n",
       "       [0.91444799, 0.9485268 ],\n",
       "       [0.9485268 , 0.9570465 ],\n",
       "       [0.9570465 , 0.96343628],\n",
       "       [0.96343628, 0.98970536]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Librispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = None\n",
    "train_splits = [\"train-clean-100\", \"train-clean-360\", \"train-other-500\"]\n",
    "dev_splits = [\"dev-clean\", \"dev-other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for split in train_splits:\n",
    "    df = load_df_from_tsv(os.path.join(root, split + '.tsv'))\n",
    "    dfs.append(df)\n",
    "combined_df = pd.concat(dfs)\n",
    "save_df_to_tsv(combined_df, os.path.join(root, 'train_ls960_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for split in dev_splits:\n",
    "    df = load_df_from_tsv(os.path.join(root, split + '.tsv'))\n",
    "    dfs.append(df)\n",
    "combined_df = pd.concat(dfs)\n",
    "save_df_to_tsv(combined_df, os.path.join(root, 'dev_ls960_asr.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Low Resource TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, '{}_st_{}.tsv'.format(split, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for duration in [0]: # in hours\n",
    "    limit = duration * 60 * 60 * 16000\n",
    "    indices = list(range(df.shape[0]))\n",
    "    sel_mask = np.zeros((df.shape[0]), dtype=bool)\n",
    "    if duration > 0:\n",
    "        random.shuffle(indices)\n",
    "        total = 0\n",
    "        for i, idx in enumerate(indices):\n",
    "            total += df['n_frames'][idx]\n",
    "            sel_mask[idx] = True\n",
    "            if total > limit:\n",
    "                break\n",
    "    st_df = df.iloc[sel_mask]\n",
    "    asr_df = df.iloc[~sel_mask]\n",
    "\n",
    "    # filter those without .pt files\n",
    "    filter_mask = np.zeros((asr_df.shape[0]), dtype=bool)\n",
    "    for i, id in enumerate(asr_df['id']):\n",
    "        pt_path = os.path.join(root, 'en-{}'.format(lang), 'data/{}/align'.format(split), '{}.pt'.format(id))\n",
    "        if not os.path.exists(pt_path):\n",
    "            filter_mask[i] = True\n",
    "    asr_df = asr_df.iloc[~filter_mask]\n",
    "\n",
    "    if duration > 0:\n",
    "        save_df_to_tsv(st_df, os.path.join(root, 'train-{}h_st.tsv'.format(duration)))\n",
    "    save_df_to_tsv(asr_df, os.path.join(root, 'train-{}h_asr.tsv'.format(duration)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225271, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215748, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df_from_tsv(os.path.join(root, 'train-1h_asr.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100 * 60 * 60 * 16000\n",
    "indices = list(range(df.shape[0]))\n",
    "sel_mask = np.zeros((df.shape[0]), dtype=bool)\n",
    "random.shuffle(indices)\n",
    "total = 0\n",
    "for i, idx in enumerate(indices):\n",
    "    total += df['n_frames'][idx]\n",
    "    sel_mask[idx] = True\n",
    "    if total > limit:\n",
    "        break\n",
    "asr_df = df.iloc[sel_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.00037677083333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_df['n_frames'].sum() / 16000 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(asr_df, os.path.join(root, 'train-1h_asr_100h.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ConST')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b19e2bae1ea557e2a235ed68e1ca6fc95eb26397d1b9313344955976d03228b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
